\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
2023-04-30 & 1.0 & First draft of VnV plan\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

%\listoftables
%\wss{Remove this section if it isn't needed}
%
%\listoffigures
%\wss{Remove this section if it isn't needed}

\newpage
\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\ \\

\noindent \citep{SRS}
%\wss{symbols, abbreviations or acronyms -- you can simply reference the SRS
%  \citep{SRS} tables, if appropriate}

\newpage

\pagenumbering{arabic}

%This document ... \wss{provide an introductory blurb and roadmap of the
%  Verification and Validation plan}


\noindent The following section provides an overview of the Verification and Validation (V\&V) Plan
for program \progname. This section explains the purpose of this document, the scope of the
system, common definitions that are used in the document. Throughout this document, we refer to the terms that have been already explained in
the document Software Requirements Specification for Program \progname.

\section{General Information}

\subsection{Summary}
\progname{} is tested for its segmentation module. The segmentation algorithm can be split into 3 parts, the descending aorta segmentation, the ascending aorta segmentation and the sagittal segmentation. \\

\noindent The descending aorta segmentation takes an original volume and a descending aorta centre coordinate as the inputs, and perform the segmentation on the volume, finally returns the result label volume indicate which voxel belongs to the descending aorta. \\

\noindent Smilarly, the ascending aorta segmentation takes an original volume, an ascending aorta centre coordinate, and the descending aorta segmentation result label volume as the inputs, and perform the segmentation on the original volume, finally returns the result label volume indicate which voxel belongs to the descending or ascending aorta. \\

\noindent Finally, the sagittal segmentation is used to fill in any missing voxel that is potentially belong to the aorta in the label volume.

%\wss{Say what software is being tested.  Give its name and a brief overview of
%  its general functions.}

\subsection{Objectives}

The main objective of the verification and validation is to ensure the segmentation algorithm correctness, while optimizing the algorithm for the readability and the efficiency.
The segmentation algorithm has been tested with 6 samples and the results are verified by the domain experts. Continuous integration testing can ensure the software correctness for each modification of the existing code.

%\wss{State what is intended to be accomplished.  The objective will be around
%  the qualities that are most important for your project.  You might have
%  something like: ``build confidence in the software correctness,''
%  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
%  just those that are most important.}

\subsection{Relevant Documentation}

%\wss{Reference relevant documentation.  This will definitely include your SRS
%  and your other project documents (MG, MIS, etc).  You can include these even
%  before they are written, since by the time the project is done, they will be
%  written.}

\citet{MG}
\citet{SRS}


\section{Plan}
This section provides a description of the software that is being tested, the team that will
perform the testing, and the milestones for the testing phase/
%\wss{Introduce this section.   You can provide a roadmap of the sections to
%  come.}

\subsection{Verification and Validation Team}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{Name} & \textbf{Role description}\\
  \midrule 
  \authname{} & Design and implement testing modules.\\
  Dr. Spencer Smith & Code review and provide test cases.\\
  \bottomrule
\end{tabular}\\ \\

%\wss{You, your classmates and the course instructor.  Maybe your supervisor.
%  You shoud do more than list names.  You should say what each person's role is
%  for the project.  A table is a good way to summarize this information.}

\subsection{SRS Verification Plan}

This section provides the functional requirements, the business tasks that the software is
expected to complete, and the nonfunctional requirements, the qualities that the software is
expected to exhibit.

\subsubsection{Functional Requirements}
The functional requirements have been already explained in the Software Requirements Specifications for Program \progname{}. As a reference, the names of the requirements are as follows:
\begin{itemize}
\item {Input}
\item {Region of Interest}
\item {Output}
\item {Visualization}
\end{itemize} 


\subsubsection{Nonfunctional Requirements}
Considering the use of this program in the research and academia nowadays, as well as keeping an eye on its future use in the medicine and clinical practice, the priority nonfunctional
requirements are correctness, understandability, reliability, and usability. The usability Nonfunctional requirements are as follow:
\begin{itemize}
\item {Interface}
\item {3D Rendering}
\end{itemize} 


%\wss{List any approaches you intend to use for SRS verification.  This may just
%  be ad hoc feedback from reviewers, like your classmates, or you may have
%  something more rigorous/systematic in mind..}

% \wss{Remember you have an SRS checklist}

\subsection{Design Verification Plan}

The GitHub issues lists the requirements and smaller tasks to fullfil the goal of the software requirements.
%\wss{Plans for design verification}
%
%\wss{The review will include reviews by your classmates}
%
%\wss{Remember you have MG and MIS checklists}

\subsection{Implementation Verification Plan}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static verification of
  the implementation.  Potential techniques include code walkthroughs, code
  inspection, static analyzers, etc.}

\subsection{Automated Testing and Verification Tools}

%\wss{What tools are you using for automated testing.  Likely a unit testing
%  framework and maybe a profiling tool, like ValGrind.  Other possible tools
%  include a static analyzer, make, continuous integration tools, test coverage
%  tools, etc.  Explain your plans for summarizing code coverage metrics.
%  Linters are x`another important class of tools.  For the programming language
%  you select, you should look at the available linters.  There may also be tools
%  that verify that coding standards have been respected, like flake9 for
%  Python.}

%\wss{The details of this section will likely evolve as you get closer to the
%  implementation.}
We use GitHub Actions to implement automated tests. GitHub Actions is a new feature to automate workflows that is build a cloud instance, which can be Windows, Linux or macOS. \\

\noindent GitHub Actions reads a yml file to deploy the repository on a temporary cloud instance, then execute the listed commands in the yml file. We built two automated tests with GitHub Actions: Linter and Continous Integration tests. Lint and CI tests will be executed every time when there is new code pushed on GitHub.\\

\noindent Linter is a static code analysis tool to flag programming errors, bugs and code format. \\

\noindent Continous Integration tests ensure the software correctness by comparing the result calculated with the new code to the result calculated by the old code. The old results are verified by testers and are set to the current ground truth. If the new result is very different from the old result then there is probably something wrong in the new code.\\




\subsection{Software Validation Plan}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\section{System Test Description}
	
\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.}

\subsubsection{Area of Testing1}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}
					
How test will be performed: 
					
\item{test-id2\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.}

\wss{Tests related to usability could include conducting a usability test and
  survey.}

\subsubsection{Area of Testing1}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: 
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

%\section{Unit Test Description}
%
%\wss{Reference your MIS and explain your overall philosophy for test case
%  selection.}  
%\wss{This section should not be filled in until after the MIS has
%  been completed.}
%
%\subsection{Unit Testing Scope}
%
%\wss{What modules are outside of the scope.  If there are modules that are
%  developed by someone else, then you would say here if you aren't planning on
%  verifying them.  There may also be modules that are part of your software, but
%  have a lower priority for verification than others.  If this is the case,
%  explain your rationale for the ranking of module importance.}
%
%\subsection{Tests for Functional Requirements}
%
%\wss{Most of the verification will be through automated unit testing.  If
%  appropriate specific modules can be verified by a non-testing based
%  technique.  That can also be documented in this section.}
%
%\subsubsection{Module 1}
%
%\wss{Include a blurb here to explain why the subsections below cover the module.
%  References to the MIS would be good.  You will want tests from a black box
%  perspective and from a white box perspective.  Explain to the reader how the
%  tests were selected.}
%
%\begin{enumerate}
%
%\item{test-id1\\}
%
%Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%  be automatic}
%					
%Initial State: 
%					
%Input: 
%					
%Output: \wss{The expected result for the given inputs}
%
%Test Case Derivation: \wss{Justify the expected value given in the Output field}
%
%How test will be performed: 
%					
%\item{test-id2\\}
%
%Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%  be automatic}
%					
%Initial State: 
%					
%Input: 
%					
%Output: \wss{The expected result for the given inputs}
%
%Test Case Derivation: \wss{Justify the expected value given in the Output field}
%
%How test will be performed: 
%
%\item{...\\}
%    
%\end{enumerate}
%
%\subsubsection{Module 2}
%
%...
%
%\subsection{Tests for Nonfunctional Requirements}
%
%\wss{If there is a module that needs to be independently assessed for
%  performance, those test cases can go here.  In some projects, planning for
%  nonfunctional tests of units will not be that relevant.}
%
%\wss{These tests may involve collecting performance data from previously
%  mentioned functional tests.}
%
%\subsubsection{Module ?}
%		
%\begin{enumerate}
%
%\item{test-id1\\}
%
%Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%  be automatic}
%					
%Initial State: 
%					
%Input/Condition: 
%					
%Output/Result: 
%					
%How test will be performed: 
%					
%\item{test-id2\\}
%
%Type: Functional, Dynamic, Manual, Static etc.
%					
%Initial State: 
%					
%Input: 
%					
%Output: 
%					
%How test will be performed: 
%
%\end{enumerate}
%
%\subsubsection{Module ?}
%
%...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\end{document}